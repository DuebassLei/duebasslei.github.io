---
title: Sparkå®ç”¨æ“ä½œç¬”è®°
description: SparkåŸºç¡€æ ¸å¿ƒçŸ¥è¯†åŠå¸¸ç”¨æ“ä½œæ•´ç†
tag:
  - Spark
  - å¤§æ•°æ®
  - åˆ†å¸ƒå¼è®¡ç®—
sidebar: true
outline: [2,3,4]
lastUpdated: true
---

## ğŸ“– ç›®å½•

- [Spark åŸºç¡€æ¦‚å¿µ](#spark-åŸºç¡€æ¦‚å¿µ)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [RDDï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰](#rddå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†)
- [DataFrame](#dataframe)
- [Dataset](#dataset)
- [Spark SQL](#spark-sql)
- [Spark Streaming](#spark-streaming)
- [å¸¸ç”¨æ“ä½œ](#å¸¸ç”¨æ“ä½œ)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## Spark åŸºç¡€æ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Sparkï¼Ÿ

Apache Spark æ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„é›†ç¾¤è®¡ç®—ç³»ç»Ÿï¼Œæä¾›äº†é«˜çº§ API ç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **å¿«é€Ÿ**ï¼šå†…å­˜è®¡ç®—ï¼Œæ¯” Hadoop MapReduce å¿« 100 å€
- **æ˜“ç”¨**ï¼šæ”¯æŒ Javaã€Scalaã€Pythonã€R
- **é€šç”¨**ï¼šæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
- **å®¹é”™æ€§**ï¼šè‡ªåŠ¨å®¹é”™å’Œæ¢å¤
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ•°åƒèŠ‚ç‚¹é›†ç¾¤

### åº”ç”¨åœºæ™¯

- **æ‰¹å¤„ç†**ï¼šå¤§è§„æ¨¡æ•°æ®æ‰¹å¤„ç†
- **æµå¤„ç†**ï¼šå®æ—¶æ•°æ®æµå¤„ç†
- **æœºå™¨å­¦ä¹ **ï¼šå¤§è§„æ¨¡æœºå™¨å­¦ä¹ 
- **å›¾è®¡ç®—**ï¼šå›¾åˆ†æå’Œå¤„ç†
- **SQL åˆ†æ**ï¼šç»“æ„åŒ–æ•°æ®åˆ†æ

### å®‰è£…å’Œå¯åŠ¨

```bash
# ä¸‹è½½ Spark
wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
tar -xzf spark-3.5.0-bin-hadoop3.tgz
cd spark-3.5.0-bin-hadoop3

# å¯åŠ¨ Spark Shell
./bin/spark-shell

# å¯åŠ¨ PySpark
./bin/pyspark

# å¯åŠ¨ Spark é›†ç¾¤
./sbin/start-master.sh
./sbin/start-worker.sh spark://localhost:7077
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### RDDï¼ˆResilient Distributed Datasetï¼‰

å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼ŒSpark çš„åŸºæœ¬æ•°æ®æŠ½è±¡ã€‚

### DataFrame

ä»¥å‘½ååˆ—ç»„ç»‡çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼å…³ç³»å‹æ•°æ®åº“çš„è¡¨ã€‚

### Dataset

ç±»å‹å®‰å…¨çš„ DataFrameï¼Œç»“åˆäº† RDD å’Œ DataFrame çš„ä¼˜ç‚¹ã€‚

### SparkContext

Spark åº”ç”¨çš„å…¥å£ç‚¹ã€‚

### SparkSession

Spark SQL çš„å…¥å£ç‚¹ï¼Œç»Ÿä¸€äº† SparkContextã€SQLContext ç­‰ã€‚

### Driver

è¿è¡Œ main å‡½æ•°çš„è¿›ç¨‹ï¼Œè´Ÿè´£åˆ›å»º SparkContextã€‚

### Executor

åœ¨é›†ç¾¤èŠ‚ç‚¹ä¸Šè¿è¡Œä»»åŠ¡çš„è¿›ç¨‹ã€‚

---

## RDDï¼ˆå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼‰

### åˆ›å»º RDD

```scala
// ä»é›†åˆåˆ›å»º
val rdd = sc.parallelize(Seq(1, 2, 3, 4, 5))

// ä»æ–‡ä»¶åˆ›å»º
val rdd = sc.textFile("file:///path/to/file")

// ä» HDFS åˆ›å»º
val rdd = sc.textFile("hdfs://namenode:9000/path/to/file")
```

```python
# Python
rdd = sc.parallelize([1, 2, 3, 4, 5])
rdd = sc.textFile("file:///path/to/file")
```

### è½¬æ¢æ“ä½œï¼ˆTransformationsï¼‰

```scala
// Map
val mapped = rdd.map(x => x * 2)

// Filter
val filtered = rdd.filter(x => x > 2)

// FlatMap
val flatMapped = rdd.flatMap(x => x.split(" "))

// Distinct
val distinct = rdd.distinct()

// Union
val union = rdd1.union(rdd2)

// Intersection
val intersection = rdd1.intersection(rdd2)

// Subtract
val subtract = rdd1.subtract(rdd2)

// Cartesian
val cartesian = rdd1.cartesian(rdd2)
```

### è¡ŒåŠ¨æ“ä½œï¼ˆActionsï¼‰

```scala
// Collect
val result = rdd.collect()

// Count
val count = rdd.count()

// First
val first = rdd.first()

// Take
val take = rdd.take(10)

// Reduce
val sum = rdd.reduce((a, b) => a + b)

// Aggregate
val result = rdd.aggregate(0)(_ + _, _ + _)

// Foreach
rdd.foreach(println)
```

### Key-Value æ“ä½œ

```scala
// Map to Pair
val pairs = rdd.map(x => (x, 1))

// GroupByKey
val grouped = pairs.groupByKey()

// ReduceByKey
val reduced = pairs.reduceByKey(_ + _)

// AggregateByKey
val aggregated = pairs.aggregateByKey(0)(_ + _, _ + _)

// SortByKey
val sorted = pairs.sortByKey()

// Join
val joined = rdd1.join(rdd2)

// LeftOuterJoin
val leftJoined = rdd1.leftOuterJoin(rdd2)

// RightOuterJoin
val rightJoined = rdd1.rightOuterJoin(rdd2)

// Cogroup
val cogrouped = rdd1.cogroup(rdd2)
```

---

## DataFrame

### åˆ›å»º DataFrame

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("DataFrameExample")
  .getOrCreate()

// ä» RDD åˆ›å»º
val df = spark.createDataFrame(rdd, schema)

// ä» JSON åˆ›å»º
val df = spark.read.json("file:///path/to/file.json")

// ä» CSV åˆ›å»º
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("file:///path/to/file.csv")

// ä» Parquet åˆ›å»º
val df = spark.read.parquet("file:///path/to/file.parquet")
```

```python
# Python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

# ä» JSON åˆ›å»º
df = spark.read.json("file:///path/to/file.json")

# ä» CSV åˆ›å»º
df = spark.read.option("header", "true").csv("file:///path/to/file.csv")
```

### DataFrame æ“ä½œ

```scala
// é€‰æ‹©åˆ—
df.select("name", "age")

// è¿‡æ»¤
df.filter(df("age") > 18)

// åˆ†ç»„èšåˆ
df.groupBy("department").agg(avg("salary"), max("age"))

// æ’åº
df.orderBy(desc("age"))

// è¿æ¥
df1.join(df2, df1("id") === df2("id"), "inner")

// å»é‡
df.distinct()

// æ·»åŠ åˆ—
df.withColumn("new_col", df("age") * 2)

// åˆ é™¤åˆ—
df.drop("column_name")
```

```python
# Python
df.select("name", "age")
df.filter(df.age > 18)
df.groupBy("department").agg({"salary": "avg", "age": "max"})
df.orderBy(desc("age"))
df1.join(df2, df1.id == df2.id, "inner")
```

---

## Dataset

### åˆ›å»º Dataset

```scala
import spark.implicits._

case class Person(name: String, age: Int)

val people = Seq(Person("Alice", 30), Person("Bob", 25))
val ds = spark.createDataset(people)

// ä» DataFrame è½¬æ¢
val ds = df.as[Person]
```

### Dataset æ“ä½œ

```scala
// ç±»å‹å®‰å…¨çš„æ“ä½œ
ds.filter(_.age > 18)
ds.map(_.name)
ds.groupByKey(_.department).agg(avg(_.salary))
```

---

## Spark SQL

### åŸºæœ¬ä½¿ç”¨

```scala
// æ³¨å†Œä¸´æ—¶è¡¨
df.createOrReplaceTempView("people")

// æ‰§è¡Œ SQL
val result = spark.sql("SELECT name, age FROM people WHERE age > 18")
```

```python
# Python
df.createOrReplaceTempView("people")
result = spark.sql("SELECT name, age FROM people WHERE age > 18")
```

### çª—å£å‡½æ•°

```scala
import org.apache.spark.sql.expressions.Window

val windowSpec = Window.partitionBy("department").orderBy(desc("salary"))

df.withColumn("rank", rank().over(windowSpec))
  .withColumn("dense_rank", dense_rank().over(windowSpec))
  .withColumn("row_number", row_number().over(windowSpec))
```

---

## Spark Streaming

### åŸºæœ¬ä½¿ç”¨

```scala
import org.apache.spark.streaming._

val ssc = new StreamingContext(sparkConf, Seconds(1))

// ä» Socket æ¥æ”¶æ•°æ®
val lines = ssc.socketTextStream("localhost", 9999)

// å¤„ç†æ•°æ®
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

wordCounts.print()

ssc.start()
ssc.awaitTermination()
```

### Structured Streaming

```scala
import org.apache.spark.sql.streaming._

val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic")
  .load()

val query = df.writeStream
  .format("console")
  .outputMode("complete")
  .start()

query.awaitTermination()
```

---

## å¸¸ç”¨æ“ä½œ

### ç¼“å­˜

```scala
// ç¼“å­˜ RDD
rdd.cache()
rdd.persist(StorageLevel.MEMORY_ONLY)

// ç¼“å­˜ DataFrame
df.cache()

// å–æ¶ˆç¼“å­˜
rdd.unpersist()
```

### åˆ†åŒº

```scala
// é‡æ–°åˆ†åŒº
rdd.repartition(10)

// åˆå¹¶åˆ†åŒº
rdd.coalesce(5)

// è‡ªå®šä¹‰åˆ†åŒº
rdd.partitionBy(new HashPartitioner(10))
```

---

## æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–

- **åˆç†åˆ†åŒº**ï¼šé¿å…æ•°æ®å€¾æ–œ
- **ä½¿ç”¨ç¼“å­˜**ï¼šå¤ç”¨è®¡ç®—ç»“æœ
- **å¹¿æ’­å˜é‡**ï¼šå‡å°‘æ•°æ®ä¼ è¾“
- **ä½¿ç”¨ DataFrame**ï¼šæ¯” RDD æ€§èƒ½æ›´å¥½

### æ•°æ®å€¾æ–œå¤„ç†

- **å¢åŠ éšæœºå‰ç¼€**ï¼šæ‰“æ•£çƒ­ç‚¹æ•°æ®
- **ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨**ï¼šå‡åŒ€åˆ†å¸ƒæ•°æ®
- **ä¸¤é˜¶æ®µèšåˆ**ï¼šå…ˆå±€éƒ¨èšåˆå†å…¨å±€èšåˆ

### å†…å­˜ç®¡ç†

- **åˆç†è®¾ç½®å†…å­˜**ï¼šexecutor å’Œ driver å†…å­˜
- **ä½¿ç”¨åºåˆ—åŒ–**ï¼šå‡å°‘å†…å­˜å ç”¨
- **åŠæ—¶é‡Šæ”¾ç¼“å­˜**ï¼šé¿å…å†…å­˜æº¢å‡º

---

##  å­¦ä¹ èµ„æº

- [Spark å®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- [Spark ç¼–ç¨‹æŒ‡å—](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [Spark SQL æŒ‡å—](https://spark.apache.org/docs/latest/sql-programming-guide.html)

---

## ğŸ’¡ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# æäº¤ Spark ä½œä¸š
spark-submit --class com.example.MainClass app.jar

# æäº¤åˆ°é›†ç¾¤
spark-submit --master spark://master:7077 --class com.example.MainClass app.jar

# è®¾ç½®èµ„æº
spark-submit --executor-memory 4g --executor-cores 2 app.jar

# æŸ¥çœ‹ä½œä¸š
# http://localhost:8080
```

